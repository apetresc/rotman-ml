{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPlQbXD8lmrbycffc04Ls4m",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/apetresc/rotman-ml/blob/master/notebooks/Rotman_AirBNB_New_User_Bookings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MLOps Colloquium - AirBNB New User Bookings\n",
        "\n",
        "This data set, from [a Kaggle competition](https://www.kaggle.com/competitions/airbnb-recruiting-new-user-bookings/overview) that AirBNB used for recruitment, contains demographic and summary data about a batch of new users, together with some data about their web sessions. The goal of the original exercise was to train a model that could predict which country a new user would make their very first booking in.\n",
        "\n",
        "We, however, are going to assume this model already exists. Instead, we're going to focus on a very common operation in real-world ML workflows: resolving data issues that can affect the accuracy of the model."
      ],
      "metadata": {
        "id": "Erx6fcmH1ugw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "YOUR_NAME = 'apetresc'\n",
        "GCP_PROJECT_ID = ''\n",
        "REGION = 'us-central1'\n",
        "GCS_BUCKET = 'gs://rotman-vertex-demo'"
      ],
      "metadata": {
        "id": "oE2u1gMy5szG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SZAZBm-4wyh0"
      },
      "outputs": [],
      "source": [
        "!pip install kfp==1.8.14 scikit-learn google-cloud-pipeline-components==1.0.24 matplotlib\n",
        "!pip install pyspark\n",
        "!wget https://github.com/GoogleCloudDataproc/hadoop-connectors/releases/download/v2.2.11/gcs-connector-hadoop3-2.2.11-shaded.jar"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import kfp\n",
        "from kfp.v2 import dsl, components\n",
        "from google_cloud_pipeline_components.v1.dataproc import \\\n",
        "    DataprocPySparkBatchOp"
      ],
      "metadata": {
        "id": "58KvGm1Aw1zJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TRAINING_DATA_URI = GCS_BUCKET + \"/airbnb-new-user-bookings\""
      ],
      "metadata": {
        "id": "_XRLhVtnyFo1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The shell commands below are going to download a local copy of the dataset from GCS. You can manually inspect it (once the cell has finished running) from the Files tab on the left (in Colab)."
      ],
      "metadata": {
        "id": "17sviXe0_LG7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gsutil cp spark_code.py {GCS_BUCKET}/code/{YOUR_NAME}/spark_code.py\n",
        "!gsutil cp -r {GCS_BUCKET}/airbnb-new-user-bookings/ ./"
      ],
      "metadata": {
        "id": "PnfIU-wZ6dX4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to experiment with the data before committing it to a pipeline, we can use a local PySpark installation to grab and visualize the data."
      ],
      "metadata": {
        "id": "uB7mt_8i2ia7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import LongType, StringType, StructField, StructType, BooleanType, ArrayType, IntegerType, DateType, FloatType\n",
        "import pyspark.sql.functions as F"
      ],
      "metadata": {
        "id": "g7nkcrZI6m75"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.getOrCreate()"
      ],
      "metadata": {
        "id": "RGOUyYEO8GjK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have a Spark session, we can load the CSVs we pulled above into a DataFrame and start exploring it."
      ],
      "metadata": {
        "id": "GYjuX-wF221B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "users_df = spark.read \\\n",
        "    .option(\"header\", \"true\") \\\n",
        "    .option(\"inferSchema\", \"true\") \\\n",
        "    .csv('./airbnb-new-user-bookings/users.csv')\n",
        "users_df.show()"
      ],
      "metadata": {
        "id": "-XLINugX8Z3X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sessions_df = spark.read \\\n",
        "    .option(\"header\", \"true\") \\\n",
        "    .option(\"inferSchema\", \"true\") \\\n",
        "    .csv('./airbnb-new-user-bookings/sessions.csv')\n",
        "sessions_df.show()"
      ],
      "metadata": {
        "id": "g8GSpkuW_VFS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the course of our exploration, we detect something... unusual... about the distribution of age columns."
      ],
      "metadata": {
        "id": "Lmi81p4j3BD3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "ages = users_df.select(F.col('age').cast('int')).rdd.flatMap(lambda x: x).collect()\n",
        "plt.hist([age for age in ages if age is not None])\n",
        "plt.yscale('log')\n",
        "plt.title('Distribution of Ages')\n",
        "plt.xlabel('Age')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7FdymCiKEI5N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It seems like there's some corruption somewhere in our data pipeline. Unless AirBNB really has a few ~2000-year-old customers, we need to figure out what these incorrect values mean and try to fix them.\n",
        "\n",
        "In the cell below (with the `%%writefile` magic), write some Spark code that can pre-process the `users` table to deal with these erroneous fields.\n",
        "\n",
        "Of course, feel free to create other exploratory cells above it to help you figure out how to investigate and solve the problem, but condense it all into the `%%writefile` cell once you're done, since our final Spark job needs to be a single, self-contained Spark application, not a bunch of Jupyter notebook cells."
      ],
      "metadata": {
        "id": "CP5wM_RB67aH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile spark_code.py\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "# Your code goes here"
      ],
      "metadata": {
        "id": "zewPr-K95U_r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Putting it all together\n",
        "\n",
        "Now that you've identified and solved the data issue, we need to make sure this fix can be applied automatically to all future training and evaulation runs. Below is the Kubeflow Pipeline we're using to train this model in production.\n",
        "\n",
        "Given the pipeline below, modify it so that the data preprocessing Spark job you wrote above acts on the loaded data before it reaches the training job.\n",
        "\n",
        "Hint: Try using the `DataprocPySparkBatchOp` component we imported above!\n",
        "\n",
        "(You may find it useful to examine [this sample Notebook](https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/pipelines/google_cloud_pipeline_components_dataproc_tabular.ipynb) in detail to see some more examples of more sophisticated pipelines that make use of `DataprocPySparkBatchOp`)"
      ],
      "metadata": {
        "id": "WaKxGt1J0E4k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dsl.component(base_image=\"python:3.8-slim\")\n",
        "def build_args() -> list:\n",
        "    return [\n",
        "        \"--train-data-path\",\n",
        "        TRAINING_DATA_URI\n",
        "    ]\n",
        "\n",
        "@dsl.pipeline(name='airbnb-pipeline', description='A pipeline that loads CSV files, preprocesses the data, and trains a model')\n",
        "def airbnb_pipeline(\n",
        "        model_output_path: str\n",
        "):\n",
        "    # Define the pipeline steps\n",
        "    build_args_op = build_args()    \n",
        "\n",
        "    # Define the training step\n",
        "    train_step = kfp.components.load_component_from_text(\"\"\"\n",
        "    name: Training\n",
        "    description: This container runs a custom training routine.\n",
        "    inputs:\n",
        "    - name: trainingDataPath\n",
        "      type: JsonArray\n",
        "    outputs:\n",
        "    - name: model\n",
        "    implementation:\n",
        "      container:\n",
        "        image: rotman/training_image:v1\n",
        "        command: ['python', 'train.py', {inputValue: trainingDataPath}, {outputPath: model}]\n",
        "    \"\"\")\n",
        "    train_op = train_step(build_args_op.output)\n",
        "\n",
        "    # Connect the preprocessing step to the training step\n",
        "    train_op = train_op.after(build_args_op)\n"
      ],
      "metadata": {
        "id": "Q_yiXfVAG8un"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You don't need to actually run this pipeline (although if you're feeling *very* adventurous you could; the easiest way to get access to a live Kubeflow instance would be on your own GCP account).\n",
        "\n",
        "But let's at least compile the pipeline above into the `.json` file that would be submitted for execution. I can execute this JSON file to evaluate the solution!\n",
        "\n",
        "Feel free to inspect the generated `pipeline.json` file on the \"Files\" tab on the left sidebar and see if you can get a feel for what it represents."
      ],
      "metadata": {
        "id": "9f3glO9s83uu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import kfp.v2.compiler as compiler\n",
        "compiler.Compiler().compile(airbnb_pipeline, 'pipeline.json')\n"
      ],
      "metadata": {
        "id": "oYl139jpICcy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qFenLD2ZIgsa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}